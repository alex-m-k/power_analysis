---
title: "Preliminary power analysis for Fourw study"
# author: Alex K
date: 3/30/2020
output: 
  # prettydoc::html_pretty:
  #   theme: leonids
  #   highlight: github
  #       # keep_md: TRUE
  #   toc: true
  #   toc_depth: 2
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: true
    theme: journal
bibliography: power.bib
---

```{r setup, include=FALSE}

# Global display options
knitr::opts_chunk$set(message=FALSE, 
                      idy.opts=list(width.cutoff=60),
                      fig.align="center") 

knitr::knit_hooks$set(inline = function(x) { 
  if(!is.numeric(x)){ x }
  else{prettyNum(round(x,2), big.mark=",") } })

# Packages
library(knitr)
library(kableExtra)
library(tidyverse)
library(RColorBrewer)
library(lfe)

# Graphic print options 
plot_theme <- theme(panel.border = element_blank(), 
                    panel.grid.major = element_blank(),
                    panel.grid.minor = element_blank(), 
                    axis.line = element_line(colour = "black"),
                    legend.text=element_text(size=8), 
                    legend.title=element_blank(),
                    legend.key = element_rect(colour = NA)) +
  theme_bw()

```

# Summary

As of right now, it looks to me like we will have enough power to detect an effect, based on the two power analyses that I conduct below. That said, these analyses are based on "guesstimates" of important parameters. We will likely want to debate these parameters before reaching any conclusions. 

# Estimating our study's likely power and minimal detectable effect size 

Here, I walk through several methods for conducting power analyses. Power analysis grows more complex as our research design grows more complex. Nearly all of the models we will likely run are complex enough that no off-the-shelf method for conducting power analyses will suffice. As a preliminary step, I thus perform a power analysis on perhaps the simplest model we might run: a two-period change in a continous variable. Specifically, I look at the likely power we will have to detect an effect for a change in perceived stress, as measured in the survey instrument. In this case, we are dealing with repeated measures (pre- and post-) on individual subjects within clusters. This is already more complicated than textbook examples; below I suggest ways we could try to make the power analysis more to true to reality. 

The good news is that because we will have pre and post observations, clustering should be less of a concern than if we only had a single cross-section.

The bad news is that the standard ICC no longer applies, because it pertains to cross-sectional data. That is, we have to account not only for the time invariant effects of clusters but also the time invariant effects due to individual subjects' idiosyncracies. We therefore need a more complex ICC, which requires more parameters and/or data to be estimated.

To conduct a power analysis, there are then two ways forward:

1. Use closed-form solutions
2. Simulate our own data

Both approaches require values for parameters that are unknown to us. I obtain all parameters from existing studies, but they are really just guesses. The benefit of closed-form solutions is that, while they do not exactly mirror our study design, they require fewer parameters. On the other hand, while simulations require parameters that are not typically published, they are going to get us closer to our study design. A future step would be to get the underlying data from a study like @Kossek_2019 in order to estimate the additional parameters. 

## Using closed-form solutions

Here I conduct a power analysis using formulas provided by @McConnell_2015; [see @Teerenstra_2012; and @moyer2019correct as well]. 

### The formula

Because we will have data before and after treatment, we can use either a difference-in-differences model or a model that includes the baseline outcome value as a lagged covariate.

The minimum detectable effect (MDE) for a difference-in-differences with two periods is:

$$\delta = \sqrt{\frac{2\sigma_22(1 - r)(t_{\alpha/2} t_{\beta})^2 (1 + (m - 1)\rho)}{ mk}}$$

And for a model that includes the baseline outcome as a covariate:

$$\delta = \sqrt{\frac{2\sigma_2(1 - r)(t_{\alpha/2} t_{\beta})^2 (1 + (m - 1)\rho)}{ mk}}$$

Note that in the second equation, the coefficient before $(1-r)$ is dropped, which has the desirable affect of reducing the MDE.

The ICC is captured by $\rho$, which in this case is equal to:

$$\rho = \frac{\sigma_{c}^2 + \sigma_{ct}^2}{\sigma_{c}^2 + \sigma_{ct}^2 + \sigma_{p}^2 + \sigma_{pt}^2}$$

where $\sigma_{c}^2$ and $\sigma_{ct}^2$ are equal to the time-invariant between cluster variance and time-varying within cluster variance and $\sigma_{p}^2$ and $\sigma_{pt}^2$ are equal to the time-invariant between person variance and time-varying within person variance. These can be decomposed into cluster and person autocorrelation:

$$ 
\begin{array}
{ccc}
\rho_c = \frac{\sigma_{c}^2}{\sigma_{c}^2 + \sigma_{ct}^2} & \text{and} & \rho_p = \frac{\sigma_{p}^2}{\sigma_{p}^2 + \sigma_{pt}^2}
\end{array}
$$

The parameter $r$, the fraction of the total variance composed of by the time invariant components, is equal to:

$$r = \frac{m\rho}{1 + (m - 1)\rho}\rho_c + \frac{1-\rho}{1 + (m - 1)\rho}\rho_p$$
By conditioning on the baseline value of the outcome variable, we are netting out the time invariant component of the variance (which is large when $r$ is close to 1).

### Tuning the parameters

To use these equations to estimate an MDE, we need to set several parameters. These are:

Number of clusters, $k$:
```{r}
k <- 24
```

Number of observations in each cluster, $m$:
```{r}
m <- 122
```

Degrees of freedom, $df$:
```{r}
df <- 2 * (k - 1)
```

The significance level, $\alpha$
```{r}
alpha <- 0.05
t_alpha <- abs(qt((alpha / 2), df))
```

Power, $\beta$
```{r}
beta <- 0.80
t_beta <- abs(qt((beta / 2), df))
```

Variances. This is where things get tricky. We don't have an estimate for each variance. What I will attempt to do is set one variance based on @Kossek_2019 and then play around with the others. Note that I am not entirely sure I am using the Kossek values to correctly generate an overall variance measure. 

\setlength{\leftskip}{2cm}

Overall variance, $\sigma^2$, can, I believe, be derived using @Kossek_2019, Table 2. That is, we sum the variance of the baseline and 6-month outcome because:
$$var(x + y) = var(x) + var(y) + cov(x, y)$$
and because--and this where I am not certain--we can derive the covariance from the correlation provided in Table 2:
$$corr = \frac{cov(x, y)}{\sigma_x \sigma_y}$$
I treat $x$ as baseline perceived stress ($\mu$ = 9.51, $\sigma$ = 3.08) and $y$ as 6-months perceived stress ($\mu$ = 9.25, $\sigma$ = 2.93), and $corr$ is equal to 0.56. Hence, 

$$cov(x, y) = .56 \cdot 3.08 \cdot 2.93 = 5.053664$$
and

```{r}
sigma2 <- (3.08)^2 + (2.93)^2 + 5.053664
```

\setlength{\leftskip}{0pt}

ICC, $\rho$, which for now we will assume a set value:
```{r}
rho <- 0.02
```

The fraction of the total variance composed of by the time invariant components, $r$, which we will allow to vary because we don't have good values which to estimate it:
```{r}
r <- seq(0, 1, .01)
```

Together, these allow us to estimate the MDE, $\delta$:
```{r}
# For diff-in-diffs
delta_dd <- sqrt((2* sigma2 * 2 *(1 - r) * (t_alpha + t_beta)^2 * (1 + (m - 1) * rho)) / (m*k))

# For baseline as covariate
delta_cov <- sqrt((2* sigma2 * (1 - r) * (t_alpha + t_beta)^2 * (1 + (m - 1) * rho)) / (m*k))
```

### Estimating the MDE

We can now see how $\delta$ varies over ranges of $r$.
```{r power_plot_1, echo = FALSE}

dat <- data.frame(r, delta_dd, delta_cov)

ggplot(dat,
       aes(x = r)) +
  geom_line(aes(y=delta_dd, color="Diff-in-diffs")) +
  geom_line(aes(y=delta_cov, color= "Baseline as covariate")) +
  plot_theme +
  labs(title = "Power over range of r values",
       x = "r, time invariant variance fraction",
       y = "MDE") +
  theme(legend.title=element_blank())
```

And we can see how $\delta$ varies over ranges of $r$ and $\rho$.
```{r power_plot_2, echo = FALSE}
# Note: the below should be vectorized
# a = 0.01
rho_a <- 0.01
# b = 0.02
rho_b <- 0.02
# c = 0.03
rho_c <- 0.03
# d = 0.04
rho_d <- 0.04
# e = 0.05
rho_e <- 0.05
# f = 0.04
rho_f <- 0.1

# For diff-in-diffs
delta_dd_a <- sqrt((2* sigma2 * 2 *(1 - r) * (t_alpha + t_beta)^2 * (1 + (m - 1) * rho_a)) / (m*k))
delta_dd_b <- sqrt((2* sigma2 * 2 *(1 - r) * (t_alpha + t_beta)^2 * (1 + (m - 1) * rho_b)) / (m*k))
delta_dd_c <- sqrt((2* sigma2 * 2 *(1 - r) * (t_alpha + t_beta)^2 * (1 + (m - 1) * rho_c)) / (m*k))
delta_dd_d <- sqrt((2* sigma2 * 2 *(1 - r) * (t_alpha + t_beta)^2 * (1 + (m - 1) * rho_d)) / (m*k))
delta_dd_e <- sqrt((2* sigma2 * 2 *(1 - r) * (t_alpha + t_beta)^2 * (1 + (m - 1) * rho_e)) / (m*k))
delta_dd_f <- sqrt((2* sigma2 * 2 *(1 - r) * (t_alpha + t_beta)^2 * (1 + (m - 1) * rho_f)) / (m*k))

delta_dd_af <- data.frame(delta_dd_a,
                     delta_dd_b,
                     delta_dd_c,
                     delta_dd_d,
                     delta_dd_e,
                     delta_dd_f)
delta_dd_af <- gather(delta_dd_af, rho_dd, mde_dd, delta_dd_a:delta_dd_f)

# For pretreatment as covariate
delta_cov_a <- sqrt((2* sigma2 *(1 - r) * (t_alpha + t_beta)^2 * (1 + (m - 1) * rho_a)) / (m*k))
delta_cov_b <- sqrt((2* sigma2 *(1 - r) * (t_alpha + t_beta)^2 * (1 + (m - 1) * rho_b)) / (m*k))
delta_cov_c <- sqrt((2* sigma2 *(1 - r) * (t_alpha + t_beta)^2 * (1 + (m - 1) * rho_c)) / (m*k))
delta_cov_d <- sqrt((2* sigma2 *(1 - r) * (t_alpha + t_beta)^2 * (1 + (m - 1) * rho_d)) / (m*k))
delta_cov_e <- sqrt((2* sigma2 *(1 - r) * (t_alpha + t_beta)^2 * (1 + (m - 1) * rho_e)) / (m*k))
delta_cov_f <- sqrt((2* sigma2 *(1 - r) * (t_alpha + t_beta)^2 * (1 + (m - 1) * rho_f)) / (m*k))
  
delta_cov_af <- data.frame(delta_cov_a,
                     delta_cov_b,
                     delta_cov_c,
                     delta_cov_d,
                     delta_cov_e,
                     delta_cov_f)
delta_cov_af <- gather(delta_cov_af, rho_cov, mde_cov, delta_cov_a:delta_cov_f)

dat_af <- cbind(r, 
                delta_dd_af, 
                delta_cov_af)

dat_af$rho_dd <- as.factor(dat_af$rho_dd)
levels(dat_af$rho_dd) <- c("rho = 0.01",
                          "rho = 0.02",
                          "rho = 0.03",
                          "rho = 0.04",
                          "rho = 0.05",
                          "rho = 0.1")

ggplot(dat_af,
       aes(x = r)) +
  geom_line(aes(y=mde_dd, color="Diff-in-diffs")) +
  geom_line(aes(y=mde_cov, color= "Baseline as covariate")) +
  plot_theme +
    facet_wrap(~rho_dd) + 
  labs(title = "Power over range of r and rho values",
       x = "r, time invariant variance fraction",
       y = "MDE") +
  theme(legend.title=element_blank())



```

What does this imply for our MDE? If we assume $\rho$ is equal to 0.02 and $r$ is equal to 0.50 (which I am selecting abritrarily), we would have an MDE equal to 0.527. The standard deviation of this outcome in @Kossek_2019 is 4.809 ($=\sqrt{\sigma^2}$). If we divide the MDE by $\sigma^2$, we get 0.11, which using Cohen's -@Cohen_1992 guideline is a small standardized effect size.[^fnmde]

## Simulating our own data

An alternative approach is to generate our own dataset and see what kind of power we would have to detect hypothesized effects. This approach gives us much more flexibility to generate estimates tailored to our setting, but it requires us to come up with values for multiple parameters, and these values are not readily available. What I do here is use approximate values from relevant studies.

### Tuning the parameters

Here are the parameter and the value I selected:

* $\sigma$, overall SD: 4.809; source: see use of @Kossek_2019 above
* $\rho$, ICC: 0.02; source: upperbound on nursing home study
* $\rho_p$, person autocorrelation: 0.55; source: @Cohen_1983[^fncohn] 
* $\rho_p$, cluster autocorrelation: here I am not sure what a good value is, so I try 0.1, 0.3, 0.5, and 0.9
* $\delta$, effect size: 0.97; source: 0.2$\cdot\sigma \approx$ small effect size according to @Cohen_1992 

[^fncohn]: This value comes from measuring the perceived stress of subjects twice, separated by 6 weeks.

### Generating the data

These parameters can be plugged into a data generating function to create a mock dataset.

```{r sim_dat}
# k, 24 clusters
k <- 24

# m, number of associates in each cluster, coming from FC info
#   I left out Secaucus, Coppell, Denver, and Anchorage
m_assoc <- c(281, 134, 48, 262, 191, 197, 128, 75,
             157, 116, 41, 311, 304, 274, 282, 104,
             243, 432, 228, 141, 142, 329, 216, 189)
n <- sum(m_assoc)

# Assign treatment to cluster
treatment <- if((k %% 2) == 0){
  sample(rep(0:1, k/2))
  } else {
    c(sample(rep(0:1, k/2)), 
      sample(0:1, 1))  
  }  

# Put associates in clusters
fc <- rep(1:k, m_assoc)

# sigma, overall SD, coming from Kossek et al.
sigma <- 4.809

# Generate cluster variance, setting, in this case, rho_c equal to 0.3
rho_c <- 0.3
# Set time invariant variance
sigma_c <- rnorm(k, mean = 0, sd = sigma)
# Set variance in period 1
sigma_1 <- rnorm(k, mean = 0, sd = sigma)
# Set variance in period 2
sigma_2 <- rnorm(k, mean = 0, sd = sigma)
# Set overall variance
sigma_c1 <- (sqrt(rho_c) * sigma_c) + (sqrt(1-rho_c) * sigma_1)
sigma_c2 <- (sqrt(rho_c) * sigma_c) + (sqrt(1-rho_c) * sigma_2)

# Generate associate variance, setting rho_p equal to 0.55
rho_p <- 0.55
# Set time invariant variance
sigma_p <- rnorm(n, mean = 0, sd = sigma)
# Set variance in period 1
sigma_1 <- rnorm(n, mean = 0, sd = sigma)
# Set variance in period 2
sigma_2 <- rnorm(n, mean = 0, sd = sigma)
# Set overall variance
sigma_p1 <- (sqrt(rho_p) * sigma_p) + (sqrt(1-rho_p) * sigma_1)
sigma_p2 <- (sqrt(rho_p) * sigma_p) + (sqrt(1-rho_p) * sigma_2)

# Generate outcome values
# ICC, setting equal to 0.02
rho <- 0.02
# Baseline, with mean equal to 9.25 from Kossek et al.
y0 <- 9.5 + (sqrt(rho) * sigma_c1[fc]) + (sqrt(1-rho) * sigma_p1)
# Post-treatment, adding a 0.07 decrease for common time trend 
# This was the decrease among controls in Kossek et al. 2019
y1 <- 9.43 + (sqrt(rho) * sigma_c2[fc]) + (sqrt(1-rho) * sigma_p2)

# Add treatment effect, setting value to 0.5
y1 <- ifelse(treatment[fc] == 1,
             y1 - 0.97,
             y1)
```

```{r verify_ICCs, include = FALSE}
# Verify that ICCs were correctly set
# Generate cluster component
# Period 1
c_mu0 <- data.frame(fc, y0) %>%
  group_by(fc) %>%
  summarize(group_mean = mean(y0))
# Period 2
c_mu1 <- data.frame(fc, y1) %>%
  group_by(fc) %>%
  summarize(group_mean = mean(y1))

# rho_c estimate
# Not sure if this is working
cor(c_mu1$group_mean, c_mu0$group_mean)

# Generate associate component
p0 <- y0 - c_mu0$group_mean[fc]
p1 <- y1 - c_mu1$group_mean[fc]

# rho_p estimate
# Not sure if this is working
cor(p1, p0)
```

```{r simulate, echo = FALSE}

simulate <- function(cluster_autocorr){
  
  # k, 24 clusters
  k <- 24
  
  # m, number of associates in each cluster, coming from FC info
  #   I left out Secaucus, Coppell, Denver, and Anchorage
  m_assoc <- c(281, 134, 48, 262, 191, 197, 128, 75,
               157, 116, 41, 311, 304, 274, 282, 104,
               243, 432, 228, 141, 142, 329, 216, 189)
  n <- sum(m_assoc)
  
  # Assign treatment to cluster
  treatment <- if((k %% 2) == 0){
    sample(rep(0:1, k/2))
    } else {
      c(sample(rep(0:1, k/2)), 
        sample(0:1, 1))  
    }  
  
  # Put associates in clusters
  fc <- rep(1:k, m_assoc)
  
  # sigma, overall SD, coming from Kossek et al.
  sigma <- 4.809
  
  # Generate cluster variance
  rho_c <- cluster_autocorr
  # Set time invariant variance
  sigma_c <- rnorm(k, mean = 0, sd = sigma)
  # Set variance in period 1
  sigma_1 <- rnorm(k, mean = 0, sd = sigma)
  # Set variance in period 2
  sigma_2 <- rnorm(k, mean = 0, sd = sigma)
  # Set overall variance
  sigma_c1 <- (sqrt(rho_c) * sigma_c) + (sqrt(1-rho_c) * sigma_1)
  sigma_c2 <- (sqrt(rho_c) * sigma_c) + (sqrt(1-rho_c) * sigma_2)
  
  # Generate associate variance, setting rho_p equal to 0.55
  rho_p <- 0.55
  # Set time invariant variance
  sigma_p <- rnorm(n, mean = 0, sd = sigma)
  # Set variance in period 1
  sigma_1 <- rnorm(n, mean = 0, sd = sigma)
  # Set variance in period 2
  sigma_2 <- rnorm(n, mean = 0, sd = sigma)
  # Set overall variance
  sigma_p1 <- (sqrt(rho_p) * sigma_p) + (sqrt(1-rho_p) * sigma_1)
  sigma_p2 <- (sqrt(rho_p) * sigma_p) + (sqrt(1-rho_p) * sigma_2)

  # Generate outcome values
  # ICC, setting equal to 0.02
  rho <- 0.02
  # Baseline, with mean equal to 9.25 from Kossek et al.
  y0 <- 9.5 + (sqrt(rho) * sigma_c1[fc]) + (sqrt(1-rho) * sigma_p1)
  # Post-treatment, adding a 0.07 decrease for common time trend 
  # This was the decrease among controls in Kossek et al. 2019
  y1 <- 9.43 + (sqrt(rho) * sigma_c2[fc]) + (sqrt(1-rho) * sigma_p2)
  
  # Add treatment effect, setting value to 0.5
  y1 <- ifelse(treatment[fc] == 1,
               y1 - 0.97,
               y1)
    
  # Create table
  dat_dd <- data.frame(y = c(y0, y1), 
                fc = rep(fc, 2),
                assoc = rep(1:n, 2),
                period = rep(0:1, c(n, n)),
                d = rep(treatment[fc], 2))
  
  dat_cov <- data.frame(y0 = y0,
                        y1 = y1,
                        fc = fc,
                        assoc = 1:n,
                        d = treatment[fc])
  
  return(list("dd" = dat_dd, "cov" = dat_cov))
}

power_test <- function(sims, cluster_autocorr){
  n.sims <- sims
  signif <- cbind("dd" = rep(NA, n.sims),
                  "cov" = rep(NA, n.sims))
  for(s in 1:n.sims){
    gen <- simulate(cluster_autocorr)
    felm.dd <- felm(y~d*period | 0 | 0 | fc, gen$dd)
    # beta.hat <- coef(felm.dd)["d:period"]
    # beta.se <- felm.dd$cse["d:period"]
    # signif[s] <- ifelse((beta.hat - 2*beta.se) > 0,
    #                     1,
    #                     0)
    signif[s, "dd"] <- ifelse(felm.dd$pval["d:period"] < 0.05,
                        1,
                        0)
    felm.cov <- felm(y1~d+y0 | 0 | 0 | fc, gen$cov)
    signif[s, "cov"] <- ifelse(felm.cov$pval["d"] < 0.05,
                        1,
                        0)
  }
  power.dd <- mean(signif[, "dd"])
  power.cov <- mean(signif[, "cov"])
  return(c(power.dd, power.cov))
}

simulations <- 1000

results  <- mapply(power_test, 
                   sim = simulations, 
                   cluster_autocorr = c(0.1, 0.3, 0.5, 0.9))
```

### Power analysis via simulation

We can then run a simulation to determine how much power we have to detect an effect at the 0.05 significance level. The basic procedure is to estimate the effect size on each simulated dataset using regression and then to count the share of times the effect size's p-value is less than 0.05.

I run `r simulations` simulations and find that we have the following power levels for each of the regression models:

```{r, echo = FALSE}

print_results <- cbind("$\\rho$" = rep(c(0.1, 0.3, 0.5, 0.9), 2),
                       "Power" = c(results[1, ], 
                                       results[2, ]))

kable(print_results, booktabs = T) %>%
    kable_styling() %>%
    pack_rows("Difference-in-differences", 1, 4) %>%
    pack_rows("Baseline as covariate", 5, 8) %>%
  add_footnote("These values are derived from regression models that cluster standard errors at the building level")
```
This leads us to conclude that we have sufficient power.

# Next steps/things to consider

* We could look at existing studies to come up with guesses for $\rho_p$ and $\rho_c$, which would allow us to finetune the value of $r$.
* Note that Dennerlein's method in the grant application would entail finding an added estimate of variance (at the department level) and that the equation he appears to have used is for cross-sectional data in a single time period.
* The treatment effect was insignificant in @Kossek_2019, so I'm not sure what a legitimate effect size would be for perceived stress. I've gotten around this in the simulation by using a standardized effect size.
* We could play around with the number of associates in each time period to see how attrition would affect the results.
* Can we use the raw data from a study to calculate all the $\rho$'s we need?

[^fnmde]: I could use a second set of eyes to confirm that the MDE I have generated ($\delta$) is not already standardized. In other words, is the MDE of 0.527 in units of perceived stress or in standard deviations?

## References
